{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Analysis\n",
    "\n",
    "This notebook provides descriptive analysis of the arXiv corpus stored in MongoDB.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nfrom datetime import datetime\nfrom collections import Counter\n\n# Add src to path if running from notebooks directory\nsys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pymongo import MongoClient\n\n# Configure display\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', 100)\n\n# Set seaborn style\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"Libraries loaded successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "MONGODB_URI = os.environ.get('MONGODB_URI', 'mongodb://localhost:27017')\n",
    "DB_NAME = os.environ.get('ARXIV_CORPUS_DB', 'arxiv_corpus')\n",
    "\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[DB_NAME]\n",
    "\n",
    "# List collections\n",
    "print(f\"Connected to database: {DB_NAME}\")\n",
    "print(f\"Collections: {db.list_collection_names()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus Overview\n",
    "\n",
    "Basic statistics about the corpus size and composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection counts\n",
    "overview = {\n",
    "    'Papers': db.papers.count_documents({}),\n",
    "    'Paragraphs': db.paragraphs.count_documents({}),\n",
    "    'Tables': db.tables.count_documents({}) if 'tables' in db.list_collection_names() else 0,\n",
    "    'Figures': db.figures.count_documents({}) if 'figures' in db.list_collection_names() else 0,\n",
    "    'Search Results': db.search_results.count_documents({}),\n",
    "    'Term Lists': db.term_lists.count_documents({}) if 'term_lists' in db.list_collection_names() else 0,\n",
    "}\n",
    "\n",
    "overview_df = pd.DataFrame(list(overview.items()), columns=['Collection', 'Count'])\n",
    "print(\"=== Corpus Overview ===\")\n",
    "display(overview_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Papers by processing status\nstatus_pipeline = [\n    {'$group': {'_id': '$status', 'count': {'$sum': 1}}},\n    {'$sort': {'count': -1}}\n]\n\nstatus_counts = list(db.papers.aggregate(status_pipeline))\nstatus_df = pd.DataFrame(status_counts).rename(columns={'_id': 'Status', 'count': 'Count'})\n\nprint(\"\\n=== Papers by Processing Status ===\")\ndisplay(status_df)\n\nif len(status_df) > 0:\n    fig, ax = plt.subplots(figsize=(8, 5))\n    sns.barplot(data=status_df, y='Status', x='Count', ax=ax, palette='Blues_d')\n    ax.set_xlabel('Number of Papers')\n    ax.set_title('Papers by Processing Status')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Papers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load papers into DataFrame\n",
    "papers_cursor = db.papers.find({}, {\n",
    "    'arxiv_id': 1,\n",
    "    'title': 1,\n",
    "    'authors': 1,\n",
    "    'categories': 1,\n",
    "    'published_date': 1,\n",
    "    'status': 1,\n",
    "    'occurrence_count': 1,\n",
    "    'search_queries': 1,\n",
    "    'document_metrics': 1,\n",
    "})\n",
    "\n",
    "papers_df = pd.DataFrame(list(papers_cursor))\n",
    "\n",
    "if len(papers_df) > 0:\n",
    "    # Extract author count\n",
    "    papers_df['author_count'] = papers_df['authors'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # Extract primary category\n",
    "    papers_df['primary_category'] = papers_df['categories'].apply(\n",
    "        lambda x: x[0] if isinstance(x, list) and len(x) > 0 else 'unknown'\n",
    "    )\n",
    "    \n",
    "    # Extract year/month from published_date\n",
    "    papers_df['published_date'] = pd.to_datetime(papers_df['published_date'], errors='coerce')\n",
    "    papers_df['year'] = papers_df['published_date'].dt.year\n",
    "    papers_df['month'] = papers_df['published_date'].dt.to_period('M')\n",
    "    \n",
    "    print(f\"Loaded {len(papers_df)} papers\")\n",
    "    display(papers_df[['arxiv_id', 'title', 'primary_category', 'year', 'status']].head(10))\n",
    "else:\n",
    "    print(\"No papers found in database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Papers by category\nif len(papers_df) > 0:\n    category_counts = papers_df['primary_category'].value_counts().head(15).reset_index()\n    category_counts.columns = ['Category', 'Count']\n    \n    print(\"=== Papers by Primary Category (Top 15) ===\")\n    display(category_counts)\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.barplot(data=category_counts, y='Category', x='Count', ax=ax, palette='viridis')\n    ax.set_xlabel('Number of Papers')\n    ax.set_title('Papers by Primary arXiv Category')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Papers by publication year\nif len(papers_df) > 0 and papers_df['year'].notna().any():\n    year_counts = papers_df['year'].value_counts().sort_index().reset_index()\n    year_counts.columns = ['Year', 'Count']\n    year_counts['Year'] = year_counts['Year'].astype(int)\n    \n    print(\"=== Papers by Publication Year ===\")\n    display(year_counts)\n    \n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(data=year_counts, x='Year', y='Count', ax=ax, palette='Blues_d')\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Number of Papers')\n    ax.set_title('Papers by Publication Year')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Papers by month (recent trend)\nif len(papers_df) > 0 and papers_df['month'].notna().any():\n    month_counts = papers_df['month'].value_counts().sort_index().tail(24)  # Last 24 months\n    \n    if len(month_counts) > 0:\n        month_df = month_counts.reset_index()\n        month_df.columns = ['Month', 'Count']\n        month_df['Month'] = month_df['Month'].astype(str)\n        \n        fig, ax = plt.subplots(figsize=(12, 5))\n        sns.lineplot(data=month_df, x='Month', y='Count', ax=ax, marker='o', color='steelblue')\n        ax.set_xlabel('Month')\n        ax.set_ylabel('Number of Papers')\n        ax.set_title('Papers by Publication Month (Last 24 Months)')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Author count distribution\nif len(papers_df) > 0:\n    print(\"=== Author Count Statistics ===\")\n    print(papers_df['author_count'].describe())\n    \n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.histplot(data=papers_df, x='author_count', bins=range(0, min(20, papers_df['author_count'].max() + 2)), \n                 ax=ax, color='steelblue', edgecolor='white')\n    ax.set_xlabel('Number of Authors')\n    ax.set_ylabel('Number of Papers')\n    ax.set_title('Distribution of Author Count per Paper')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Papers by occurrence count (how many search queries found each paper)\nif len(papers_df) > 0 and 'occurrence_count' in papers_df.columns:\n    occurrence_counts = papers_df['occurrence_count'].value_counts().sort_index().head(20).reset_index()\n    occurrence_counts.columns = ['Occurrence', 'Count']\n    \n    print(\"=== Papers by Search Query Occurrence ===\")\n    print(f\"Papers found by only 1 query: {papers_df[papers_df['occurrence_count'] == 1].shape[0]}\")\n    print(f\"Papers found by 2+ queries: {papers_df[papers_df['occurrence_count'] >= 2].shape[0]}\")\n    print(f\"Papers found by 5+ queries: {papers_df[papers_df['occurrence_count'] >= 5].shape[0]}\")\n    \n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.barplot(data=occurrence_counts, x='Occurrence', y='Count', ax=ax, palette='Blues_d')\n    ax.set_xlabel('Number of Queries Finding Paper')\n    ax.set_ylabel('Number of Papers')\n    ax.set_title('Papers by Search Query Occurrence Count')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Metrics Analysis\n",
    "\n",
    "Analysis of document structure metrics from Docling conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract document metrics\n",
    "if len(papers_df) > 0 and 'document_metrics' in papers_df.columns:\n",
    "    # Filter papers with metrics\n",
    "    papers_with_metrics = papers_df[papers_df['document_metrics'].notna()].copy()\n",
    "    \n",
    "    if len(papers_with_metrics) > 0:\n",
    "        # Expand metrics into columns\n",
    "        metrics_df = pd.json_normalize(papers_with_metrics['document_metrics'])\n",
    "        metrics_df['arxiv_id'] = papers_with_metrics['arxiv_id'].values\n",
    "        \n",
    "        print(f\"Papers with document metrics: {len(metrics_df)}\")\n",
    "        print(\"\\n=== Document Metrics Summary ===\")\n",
    "        display(metrics_df.describe())\n",
    "    else:\n",
    "        print(\"No papers have document metrics yet (run 'arxiv-corpus process convert' first)\")\n",
    "        metrics_df = pd.DataFrame()\n",
    "else:\n",
    "    print(\"No document metrics available\")\n",
    "    metrics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Document metrics distributions\nif len(metrics_df) > 0 and 'num_pages' in metrics_df.columns:\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # Pages\n    sns.histplot(data=metrics_df, x='num_pages', bins=30, ax=axes[0, 0], color='steelblue', edgecolor='white')\n    axes[0, 0].set_title('Distribution of Page Count')\n    axes[0, 0].set_xlabel('Pages')\n    \n    # Tables\n    if 'num_tables' in metrics_df.columns:\n        sns.histplot(data=metrics_df, x='num_tables', bins=20, ax=axes[0, 1], color='steelblue', edgecolor='white')\n        axes[0, 1].set_title('Distribution of Table Count')\n        axes[0, 1].set_xlabel('Tables')\n    \n    # Figures\n    if 'num_figures' in metrics_df.columns:\n        sns.histplot(data=metrics_df, x='num_figures', bins=20, ax=axes[1, 0], color='steelblue', edgecolor='white')\n        axes[1, 0].set_title('Distribution of Figure Count')\n        axes[1, 0].set_xlabel('Figures')\n    \n    # Word count\n    if 'word_count' in metrics_df.columns:\n        sns.histplot(data=metrics_df, x='word_count', bins=30, ax=axes[1, 1], color='steelblue', edgecolor='white')\n        axes[1, 1].set_title('Distribution of Word Count')\n        axes[1, 1].set_xlabel('Words')\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search Queries Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load search results\n",
    "search_results = list(db.search_results.find({}))\n",
    "search_df = pd.DataFrame(search_results)\n",
    "\n",
    "if len(search_df) > 0:\n",
    "    print(f\"Total search queries executed: {len(search_df)}\")\n",
    "    print(f\"Total results across all queries: {search_df['total_results'].sum()}\")\n",
    "    print(f\"Average results per query: {search_df['total_results'].mean():.1f}\")\n",
    "    \n",
    "    display(search_df[['query', 'base_term', 'attribute', 'domain', 'total_results']].head(10))\n",
    "else:\n",
    "    print(\"No search queries executed yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results by domain\n",
    "if len(search_df) > 0 and 'domain' in search_df.columns:\n",
    "    domain_stats = search_df.groupby('domain').agg({\n",
    "        'total_results': ['sum', 'mean', 'count']\n",
    "    }).round(1)\n",
    "    domain_stats.columns = ['Total Results', 'Avg per Query', 'Num Queries']\n",
    "    domain_stats = domain_stats.sort_values('Total Results', ascending=False)\n",
    "    \n",
    "    print(\"\\n=== Results by Domain ===\")\n",
    "    display(domain_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results by attribute\n",
    "if len(search_df) > 0 and 'attribute' in search_df.columns:\n",
    "    attr_stats = search_df.groupby('attribute').agg({\n",
    "        'total_results': ['sum', 'mean', 'count']\n",
    "    }).round(1)\n",
    "    attr_stats.columns = ['Total Results', 'Avg per Query', 'Num Queries']\n",
    "    attr_stats = attr_stats.sort_values('Total Results', ascending=False)\n",
    "    \n",
    "    print(\"\\n=== Results by Attribute ===\")\n",
    "    display(attr_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap: Domain x Attribute\nif len(search_df) > 0 and 'domain' in search_df.columns and 'attribute' in search_df.columns:\n    pivot = search_df.pivot_table(\n        values='total_results', \n        index='attribute', \n        columns='domain', \n        aggfunc='sum',\n        fill_value=0\n    )\n    \n    if pivot.shape[0] > 0 and pivot.shape[1] > 0:\n        fig, ax = plt.subplots(figsize=(12, 8))\n        sns.heatmap(pivot, annot=True, fmt='d', cmap='Blues', ax=ax, \n                    linewidths=0.5, cbar_kws={'label': 'Total Results'})\n        ax.set_title('Search Results: Attribute Ã— Domain')\n        ax.set_xlabel('Domain')\n        ax.set_ylabel('Attribute')\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Paragraphs Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paragraph statistics\npara_count = db.paragraphs.count_documents({})\n\nif para_count > 0:\n    # Element type distribution\n    element_pipeline = [\n        {'$group': {'_id': '$element_type', 'count': {'$sum': 1}}},\n        {'$sort': {'count': -1}}\n    ]\n    element_counts = list(db.paragraphs.aggregate(element_pipeline))\n    element_df = pd.DataFrame(element_counts).rename(columns={'_id': 'Element Type', 'count': 'Count'})\n    \n    print(f\"Total paragraphs: {para_count}\")\n    print(\"\\n=== Paragraphs by Element Type ===\")\n    display(element_df)\n    \n    if len(element_df) > 0:\n        fig, ax = plt.subplots(figsize=(10, 5))\n        sns.barplot(data=element_df, y='Element Type', x='Count', ax=ax, palette='viridis')\n        ax.set_xlabel('Count')\n        ax.set_title('Document Elements by Type')\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(\"No paragraphs found (run 'arxiv-corpus process extract' first)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paragraphs per paper\nif para_count > 0:\n    para_per_paper = list(db.paragraphs.aggregate([\n        {'$group': {'_id': '$arxiv_id', 'count': {'$sum': 1}}},\n        {'$sort': {'count': -1}}\n    ]))\n    \n    para_per_paper_df = pd.DataFrame(para_per_paper)\n    \n    print(\"=== Paragraphs per Paper ===\")\n    print(f\"Mean: {para_per_paper_df['count'].mean():.1f}\")\n    print(f\"Median: {para_per_paper_df['count'].median():.1f}\")\n    print(f\"Max: {para_per_paper_df['count'].max()}\")\n    \n    fig, ax = plt.subplots(figsize=(10, 5))\n    sns.histplot(data=para_per_paper_df, x='count', bins=30, ax=ax, color='steelblue', edgecolor='white')\n    ax.set_xlabel('Number of Paragraphs')\n    ax.set_ylabel('Number of Papers')\n    ax.set_title('Distribution of Paragraphs per Paper')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section title analysis\n",
    "if para_count > 0:\n",
    "    section_pipeline = [\n",
    "        {'$match': {'section_title': {'$ne': None}}},\n",
    "        {'$group': {'_id': '$section_title', 'count': {'$sum': 1}}},\n",
    "        {'$sort': {'count': -1}},\n",
    "        {'$limit': 20}\n",
    "    ]\n",
    "    section_counts = list(db.paragraphs.aggregate(section_pipeline))\n",
    "    \n",
    "    if section_counts:\n",
    "        section_df = pd.DataFrame(section_counts).rename(columns={'_id': 'Section', 'count': 'Count'})\n",
    "        print(\"\\n=== Most Common Section Titles (Top 20) ===\")\n",
    "        display(section_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Term Hits Analysis\n",
    "\n",
    "Analysis of term search results in paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraphs with term hits\n",
    "if para_count > 0:\n",
    "    hit_pipeline = [\n",
    "        {'$match': {'total_hits': {'$gt': 0}}},\n",
    "        {'$group': {'_id': None, 'count': {'$sum': 1}, 'total_hits': {'$sum': '$total_hits'}}}\n",
    "    ]\n",
    "    hit_stats = list(db.paragraphs.aggregate(hit_pipeline))\n",
    "    \n",
    "    if hit_stats:\n",
    "        print(f\"Paragraphs with term hits: {hit_stats[0]['count']}\")\n",
    "        print(f\"Total term hits: {hit_stats[0]['total_hits']}\")\n",
    "    else:\n",
    "        print(\"No term hits recorded (run term search analysis first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common terms hit\n",
    "if para_count > 0:\n",
    "    term_pipeline = [\n",
    "        {'$unwind': '$hits'},\n",
    "        {'$group': {'_id': '$hits.term', 'total_count': {'$sum': '$hits.count'}, 'paragraphs': {'$sum': 1}}},\n",
    "        {'$sort': {'total_count': -1}},\n",
    "        {'$limit': 20}\n",
    "    ]\n",
    "    term_hits = list(db.paragraphs.aggregate(term_pipeline))\n",
    "    \n",
    "    if term_hits:\n",
    "        term_df = pd.DataFrame(term_hits).rename(columns={\n",
    "            '_id': 'Term', \n",
    "            'total_count': 'Total Hits',\n",
    "            'paragraphs': 'Paragraphs'\n",
    "        })\n",
    "        print(\"\\n=== Most Frequent Terms in Corpus (Top 20) ===\")\n",
    "        display(term_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'Report Generated': datetime.now().isoformat(),\n",
    "    'Database': DB_NAME,\n",
    "    'Total Papers': db.papers.count_documents({}),\n",
    "    'Papers Downloaded': db.papers.count_documents({'status': 'downloaded'}),\n",
    "    'Papers Converted': db.papers.count_documents({'status': 'converted'}),\n",
    "    'Papers with Embeddings': db.papers.count_documents({'status': 'embedded'}),\n",
    "    'Papers Processed': db.papers.count_documents({'status': 'processed'}),\n",
    "    'Papers with Errors': db.papers.count_documents({'status': 'error'}),\n",
    "    'Total Paragraphs': db.paragraphs.count_documents({}),\n",
    "    'Total Search Queries': db.search_results.count_documents({}),\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(list(summary.items()), columns=['Metric', 'Value'])\n",
    "print(\"=== Corpus Summary Report ===\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "client.close()\n",
    "print(\"MongoDB connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}