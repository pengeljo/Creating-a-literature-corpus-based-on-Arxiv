Learning Affordance Landscapes for Interaction Exploration in 3D Environments


Introduction


The ability to interact with the environment is an essential skill for
embodied agents operating in human spaces. Interaction gives agents
the capacity to modify their environment, allowing them to move from
semantic navigation tasks (e.g., &quot;go to the kitchen; find the coffee
cup&quot;) towards complex tasks involving interactions with their
surroundings (e.g., &quot;heat some coffee and bring it to me&quot;).


Today&apos;s embodied agents are typically trained to perform specific
interactions in a supervised manner. For example, an agent learns to
navigate to specified objects *REF*, a dexterous hand
learns to solve a Rubik&apos;s cube *REF*, a robot learns
to manipulate a rope *REF*. In these cases and many
others, it is known a priori what objects are relevant for the
interactions and what the goal of the interaction is, whether
expressed through expert demonstrations or a reward crafted to elicit
the desired behavior. Despite exciting results, the resulting agents
remain specialized to the target interactions and objects for which
they were taught.


In contrast, we envision embodied agents that can enter a novel 3D
environment, move around to encounter new objects, and autonomously
discern the affordance landscape---what are the interactable objects,
what actions are relevant to use them, and under what conditions will
these interactions succeed? Such an agent could then enter a new
kitchen (say), and be primed to address tasks like &quot;wash my coffee cup
in the sink.&quot; These capabilities would mimic humans&apos; ability to
efficiently discover the functionality of even unfamiliar objects
though a mixture of learned visual priors and exploratory
manipulation.


To this end, we introduce the exploration for interaction problem: a
mobile agent in a 3D environment must autonomously discover the
objects with which it can physically interact, and what actions are
valid as interactions with them.


FIGURE 1


Exploring for interaction presents a challenging search problem over
the product of all objects, actions, agent positions, and action
histories. Furthermore, many objects are hidden (e.g., in drawers) and
need to be discovered, and their interaction dynamics are not
straightforward (e.g., cannot open an already opened door, can only
slice an apple if a knife is picked up). In contrast, exploration
for navigating a static environment involves relatively small action
spaces and dynamics governed solely by the presence/absence of
obstacles *REF*.


Towards addressing these challenges, we propose a deep reinforcement
learning (RL) approach in which the agent discovers the affordance
landscape of a new, unmapped 3D environment. The result is a strong
prior for where to explore and what interactions to try. Specifically,
we consider an agent equipped with an egocentric RGB-D camera and an
action space comprised of navigation and manipulation actions (turn
left, open, toggle, etc.), whose effects are initially unknown to the
agent. We reward the agent for quickly interacting with all objects in
an environment. In parallel, we train an affordance model online to
segment images according to the likelihoods for each of the agent&apos;s
actions succeeding there, using the partially observed interaction
data generated by the exploration policy. The two models work in
concert to functionally explore the environment. See Figure *REF*.


Our experiments with AI2-iTHOR *REF* demonstrate the
advantages of interaction exploration. Our agents can quickly seek out
new objects to interact with in new environments, matching the performance of the best exploration method in 42% fewer timesteps and
surpassing them to discover 1.33× more interactions when fully trained. Further, we show our agent and
affordance model help train multi-step interaction policies (e.g., washing objects at a sink),
improving success rates by up to 16% on various tasks, with fewer
training samples, despite sparse rewards and no human demonstrations.


Related Work


Visual affordances An affordance is the potential for action
*REF*. In computer vision, visual affordances are
explored in various forms: predicting where to grasp an object from
images and video *REF*, inferring how people might use a space *REF*,
*REF* or tool *REF*, and priors for human body poses *REF*, *REF*,
*REF*, *REF*. Our work offers a new
perspective on learning visual affordances. Rather than learn them
passively from a static dataset, the proposed agent actively seeks new
affordances via exploratory interactions with a dynamic environment.
Furthermore, unlike prior work, our approach yields not just an image
model, but also a policy for exploring interactions, which we show
accelerates learning new downstream tasks for an embodied agent.


Exploration for navigation in 3D environments Recent embodied AI
work in 3D simulators *REF*, *REF*, *REF*, *REF* tackles navigation: the agent
moves intelligently in an unmapped but static environment to reach a
goal (e.g., *REF*, *REF*, *REF*, *REF*). Exploration policies for
visual navigation efficiently map the environment in an unsupervised
&quot;preview&quot; stage *REF*, *REF*, *REF*, *REF*, *REF*,
*REF*. The agent is rewarded for maximizing the area
covered in its inferred occupancy map *REF*, *REF*, *REF*, the novelty of the states
visited *REF*, pushing the frontier of explored areas *REF*, and related metrics *REF*.
For a game setting in VizDoom, classic frontier-based exploration is
improved by learning the visual appearance of hazardous regions (e.g.,
enemies, lava) where the agent&apos;s health score has previously declined *REF*.


In contrast to all the above, we study the problem of exploration for
interaction in dynamic environments where the agent can modify the
environment state (open/close doors, pick up objects etc.). Our
end goal is not to build a top-down occupancy
map, but rather to quickly interact with as many objects as possible
in a new environment. In other words, whereas exploration for
navigation promotes rapidly completing a static environment map,
exploration for interaction promotes rapidly completing the agent&apos;s
understanding of its interactions in a dynamic environment.


Interaction in 3D environments Beyond navigation, recent work
leverages simulated interactionbased environments
*REF*, *REF*, *REF*, *REF* to develop agents that can also perform actions
(e.g., moving objects, opening doors) with the goal of eventually
translating policies to real robots *REF*, *REF*. These tasks include answering questions (&quot;how
many apples are in the fridge?&quot;) that may require navigation *REF* as well as interaction *REF*.
Towards service robotics, goal driven planning *REF*, instruction following *REF*, and cooking
*REF* agents are trained using imitation learning on expert trajectories.


Our idea to efficiently explore interactions is complementary.
Rather than learn a task-specific policy from demonstrations, our
approach learns task-agnostic exploration behavior from experience to
quickly discover the affordance landscape. Our model can be coupled
with a downstream task like those tackled above to accelerate their
training, as we demonstrate in the experiments.


Self-supervised interaction learning Prior work studies actively
learning manipulation policies through self-supervised training for
grasping *REF*, *REF*, *REF*, *REF*, *REF*, pushing/poking
*REF*, *REF* and drone control *REF*. Unstructured play data has also been used to
learn subgoal policies *REF*, which are then sampled
to solve complex tasks. Object affordance models are learned for
simple objects in table-top environments *REF*,
*REF* and for block pushing tasks in gridworlds
*REF*. We share the general idea of learning through
interaction; however, we focus on high-level interaction policies
requiring both navigation and manipulation (e.g., moving to the
counter and picking up knife) rather than fine-grained manipulation
policies (e.g., altering joint angles).


Intrinsic motivation In the absence of external rewards from the
environment, reinforcement learning agents can nonetheless focus their
behavior to satisfy intrinsic drives *REF*. Recent
work formulates intrinsic motivation based on curiosity
*REF*, *REF*, *REF*, novelty *REF*, *REF*, and empowerment
*REF* to improve video game playing agents (e.g.,
VizDoom, Super Mario) or increase object attention
*REF*. Our idea can be seen as a distinct form of
intrinsic motivation, where the agent is driven to experience more
interactions in the environment. Also, we focus on realistic
human-centered 3D environments, rather than video games, and with
high-level interactions that can change object state, rather than
low-level physical manipulations.


Approach


Our goal is to train an interaction exploration agent to enter a new,
unseen environment and successfully interact with all objects
present. This involves identifying the objects that are interactable,
learning to navigate to them, and discovering all valid interactions
with them (e.g., discovering that the agent can toggle a light
switch, but not a knife).


To address the challenges of a large search space and complex
interaction dynamics, our agent learns visual affordances to help it
intelligently select regions of the environment to explore and
interactions to try. Critically, our agent builds this affordance
model through its own experience interacting with the environment
during exploration. For example, by successfully opening a cupboard,
the agent learns that objects with handles are likely to be
&quot;openable\&quot;. Our method yields an interaction exploration policy that
can quickly perform object interactions in new environments, as well
as a visual affordance model that captures where each action is likely
to succeed in the egocentric view.


In the following, we first define the interaction exploration task
(Sec. [3.1]). Then, we show how an agent can train an affordance model via interaction
experience (Sec. [3.2]). Finally, we
present our policy learning architecture that integrates interaction
exploration and affordance learning, and allows transfer to
goal-driven policy learning (Sec. [3.3]).


Learning exploration policies for interaction


We want to train an agent to interact with as many objects as possible
in a new environment. Agents can perform actions from a set FORMULA, 
consisting of navigation actions AN (e.g., move
forward, turn left/right) and object interactions AI (e.g.,
take/put, open/close). The interaction exploration task is set up as a reinforcement learning
problem. The agent is spawned at an initial state s0. At each time
step t, the agent in state st receives an observation (xt,
θt) consisting of the RGB image xt and the agent&apos;s odometry θt, executes an
action at ∼ A and receives a reward FORMULA. The agent is rewarded for each
successful interaction with a new object ot: FORMULA
where c(a, o) counts how many times interaction (a, o) has
successfully occurred in the past. The goal is to learn an exploration
policy πE that maximizes this reward over an episode of length T. See Sec. [3.3] for the
policy architecture. The hard, count-based reward formulation only
rewards the agent once per interaction, incentivizing broad coverage
of interactions, rather than mastery of a few, which is useful for
downstream tasks involving arbitrary interactions.


Affordance learning via interaction exploration


As the agent explores, it attempts interactions at various locations,
only some of which succeed. These attempts partially reveal the
affordances of objects --- what interactions are possible with them
--- which we capture in a visual affordance model. An explicit model
of affordances helps the agent decide what regions to visit (e.g.,
most interactions fail at walls, so avoid them) and helps extrapolate
possible interactions with unvisited objects (e.g., opening one
cupboard suggests that other handles are &quot;openable&quot;), leading to more
efficient exploration policies.


At a high level, we train an affordance segmentation model FA to
transform an input RGB-D image into a AI-channel segmentation
map, where each channel is a H ×W map over the image indicating
regions where a particular interaction is likely to succeed. Training
samples for this model comes from the agent&apos;s interaction with the environment. For example, if it
successfully picks up a kettle, pixels around that kettle are labeled
&quot;pickup-able&quot;, and these labels are propagated to all frames where the
kettle is visible (both before and after the interaction took place),
such that affordances will be recognizable even from far away. See
Fig. *REF* (right panel).


Specifically, for a trajectory FORMULA sampled from our exploration policy, we identify time steps
t1\...tN where interactions occur (at ∈ AI). For each
interaction, the world location pt at the center of the agent&apos;s
field of view is calculated by inverse perspective projection,
and stored along with the interaction type at, and success of the
interaction zt, in memory as FORMULA. This corresponds to
&quot;marking&quot; the target of the interaction.


At the end of the episode, for each frame x in the trajectory, we
generate a corresponding segmentation mask y that highlights the
position of all markers from any action that are visible in x. For
each interaction ak, the label for each pixel in the k-th
segmentation mask slice y^k^ is calculated as: FORMULA
where Mk ⊆ M is the subset of markers corresponding to interaction
ak, pij is the world location at that pixel, d is euclidean
distance, and δ is a fixed distance threshold (20cm). In other
words, each pixel is labeled 0 or 1 for affordance k depending on
whether any marker has been placed nearby (within distance δ) at any
time along the trajectory, and is visible in the current frame. If no
markers are placed, the pixel is labeled −1 for unknown. See Fig.
*REF* (right panel). This results in a FORMULA dimension segmentation label mask per frame, which
we use to train FA.


These labels are sparse and noisy, as an interaction may fail with an
object despite being valid in other conditions (e.g., opening an
already opened cupboard). To account for this, we train two distinct
segmentation heads using these labels to minimize a combination of
cross entropy losses: FORMULA where FORMULA is the indicator function over labels. Lce
is standard cross entropy loss, but is evaluated over
a subset of pixels specified by the third argument. Classifier output
yˆA scores whether each interaction is successful at a location,
while yˆI scores general interactibility (y = −1 vs. y /=
−1). The latter acts as a measure of uncertainty to ignore regions
where markers are rarely placed, regardless of success (e.g., the
ceiling, windows). The final score output by FA is the product FORMULA.


In our experiments, we consider two variants: one that marks
interactions with a single point, and one that marks all points on the
target object of the interaction. The former translates to fixed scale
labels at the exact interaction location, supposing no prior knowledge
about object segmentation. The latter is more general and considers
the whole object as &quot;interactable&quot;, leading to denser labels.In both
cases, the object class and valid interactions are unknown to the
agent.


FIGURE


Policy learning architecture and transfer


Next we put together both pieces --- the interaction exploration
objective and the affordance segmentations --- in our policy learning
framework. We adopt an actor-critic policy model and a U-Net
*REF* architecture for affordances. At each time
step, we receive the current egocentric frame
x and generate its affordance maps yˆ = FA(x). The visual
observations and affordance maps
are encoded using a 3-layer convolutional neural network (CNN) each,
and then concatenated and merged using a fully connected layer. This is then fed to a gated
recurrent unit (GRU) recurrent neural network to aggregate
observations over time, and finally to an actor-critic network (fully
connected layers) to generate the next action distribution and value.
We train this network using PPO *REF* for 1M frames,
with rollouts of T = 256 time steps. See Fig. *REF* 
(left) and Supp for architecture details.


We train the policy network and the segmentation model iteratively. As
the agent explores, we store episodes drawn from the exploration
policy, and create an affordance segmentation dataset as per Sec.
[3.2]. We train the affordance model using this dataset, and use the updated model to
generate yˆ to further train the policy network described above. See
Supp for training schedule.


The result of this process is an interaction exploration policy πE
that can quickly master object interactions in new environments, as
well as a visual affordance model FA, which captures where
interactions will likely succeed in the current view. In addition, we
show the policy transfers to better learn downstream tasks.
Specifically, we freeze the weights of the policy network and FA,
and fine-tune only the actor-critic linear layers using the downstream
task&apos;s reward (cf. Sec. [4.2]).


Experiments


We evaluate agents&apos; ability to interact with as many objects as
possible (Sec. [4.1]) and enhance policy learning on downstream tasks (Sec. [4.2]).


Simulation environment We experiment with AI2-iTHOR
*REF* (see Fig. *REF*), since it supports
context-specific interactions that can change object states, vs.
simple physics-based interactions in other 3D indoor environments
*REF*, *REF*. We use all kitchen scenes;
kitchens are a valuable domain since many diverse interactions with
objects are possible, as also emphasized in prior work
*REF*, *REF*, *REF*. The scenes contain objects from 69 classes, each of which supports 1-5
interactions. We split the 30 scenes into training (20), validation
(5), and testing (5) sets. We randomize objects&apos; positions and states
(isOpen, isToggled etc), agent start location, and camera viewpoint
when sampling episodes.


Agents can both navigate: AN = {move forward, turn left/right
30 FORMULA, look up/down 15 FORMULA}, and perform interactions with objects
in the center of the agent&apos;s view: AI = {take, put, open, close,
toggle-on, toggle-off, slice}. While the simulator knows what actions are valid
given where the agent is, what it is holding, and what objects are
nearby, all this knowledge is hidden from the agent, who only knows if
an action succeeds or fails.


FIGURE


Baselines We compare several methods:
- [Random] selects actions uniformly at random.
[Random+] selects random navigation actions from
AN to reach unvisited locations, then cycles through all
possible object interactions in AI.
- [Curiosity] *REF*, *REF* rewards actions that lead to states the agent
cannot predict well.
- [Novelty] *REF*, *REF*, *REF* rewards visits to new, unexplored physical
locations. We augment this baseline to cycle through all
interactions upon reaching a novel location.
- [ObjCoverage] *REF*, *REF* rewards an agent for visiting new objects
(moving close to it, and centering it in view), but not for
interacting with them. We similarly augment this to cycle over all
interactions.


The above three are standard paradigms for exploration. See Supp for
details.


Ablations We examine several variants of the proposed interaction
exploration agent. All variants are rewarded for interactions with
novel objects (Equation *REF*) and use the same
architecture (Sec. [3.3]).
- INTEXP(RGB) uses only the egocentric RGB frames to learn the
policy, no affordance map.
- [IntExp(Sal)] uses RGB plus heatmaps from a
pretrained saliency model *REF* as input, which
highlight salient objects but are devoid of affordance cues.
- INTEXP(GT) uses ground truth affordances from the simulator.
- INTEXP(PT) and INTEXP(OBJ) use affordances learned
on-the-fly from interaction with the environment by marking fixed
sized points or whole objects respectively (See Sec.
[3.2]). INTEXP(PT) is our default model for experiments unless specified.


In short, RANDOM and RANDOM+ test if a learned policy is required at
all, given small and easy to navigate environments. NOVELTY,
CURIOSITY, and OBJCOVERAGE test whether intelligent interaction
policies fall out naturally from traditional exploration methods.
Finally, the interaction exploration ablations test how influential
learned visual affordances are in driving interaction discovery.


Affordance driven interaction exploration


First we evaluate how well an agent can locate and interact with all
objects in a new environment.


Metrics. For each test environment, we generate 80 randomized
episodes of 1024 time steps each. We create an &quot;oracle&quot; agent that
takes the shortest path to the next closest object and performs all
valid interactions with it, to gauge the maximum number of possible
interactions. We report (1) Coverage: the fraction of the maximum
number of interactions possible that the agent successfully performs
and (2) Precision: the fraction of interactions that the agent
attempted that were successful.


TABLE


FIGURE


Interaction exploration. Fig. *REF* (left) shows
interaction coverage on new, unseen environments over time, averaged
over all episodes and environments. See Supp for environment-specific
results. Even though CURIOSITY is trained to seek hard-to-predict
states, like the non-trained baselines it risks performing actions
that block further interaction (e.g., opening cupboards blocks paths).
RANDOM+, NOVELTY, and OBJCOVERAGE seek out new locations/objects but
can only cycle through all interactions, leading to slow discovery of
new interactions.


Our full model with learned affordance maps leads to the best
interaction exploration policies, and discovers 1.33× more unique
object interactions than the strongest baseline. Moreover, it performs
these interactions quickly --- it discovers the same number of
interactions as RANDOM+ in 63% fewer time-steps. Our method discovers
2.5× more interactions than NOVELTY at T =256.


Fig. *REF* (right) shows variants of our method that use
different visual priors. INT-EXP(RGB) has no explicit RoI model and
performs worst. In INT-EXP(SAL), saliency helps distinguish between
objects and walls/ceiling, but does not reveal what interactions are
possible with salient objects as our affordance model does. INTEXP(PT)
trains using exact interaction locations (rather than whole object
masks), thus suffering less from noisy marker labels than INTEXP(OBJ),
but it yields more conservative affordance predictions (See Fig.
*REF*).


Table *REF* shows an action-wise breakdown of coverage and
precision. In general, many objects can be opened/closed (drawers,
fridges, kettles etc.) resulting in more instances covered for those
actions. All methods rarely slice objects successfully as it requires
first locating and picking up a knife (all have cov \&lt;1%). This
requires multiple steps that are unlikely to occur randomly, and so is
overlooked by trained agents in favor of more accessible
objects/interactions. Importantly, methods that cycle through actions
eventually interact with objects, leading to moderate coverage, but
very low precision since they do not know how to prioritize
interactions. This is further exemplified in Fig. *REF*.
NOVELTY tends to seek out new locations, regardless of their potential
for interaction, resulting in few successes (green dots) and several
failed attempts (yellow dots). Our agent selectively navigates to
regions with objects that have potential for interaction. See Supp for
more examples.


Affordance prediction. In addition to exploration policies, our
method learns an affordance model. Fig. *REF* evaluates
the INTEXP agents for reconstructing the ground truth affordance
landscape of 23,637 uniformly sampled views from unseen test
environments. We report mean average precision over all interaction
classes. The ALL-ONES baseline assigns equal scores to all pixels.
INTEXP(SAL) simply repeats its saliency map AI times as the affordance
map. Other agents from Fig. *REF* do not train affordance models, thus cannot be compared. Our affordance
models learn maps tied to the individual actions of the exploring agent and result in the best
performance.


FIGURE 5 


FIGURE 6


Interaction exploration for downstream tasks


Next we fine-tune our interaction exploration agents for several
downstream tasks. The tasks are (1) RETRIEVE: The agent must take any object out of a
drawer/cabinet, and set it down in a visible location outside, (2)
STORE: The agent must take any object from outside, put it away in a
drawer/cabinet and close the door, (3) WASH: The agent must put any
object inside the sink, and turn on the tap. (4) HEAT: The agent must
put a pan/vessel on the stove-top, and turn on the burner.


These tasks have very sparse rewards, and require agents to
successfully perform multiple interactions in sequence involving
different objects. Similar tasks are studied in recent work
*REF*, *REF*, which train imitation
learning based agents on expert demonstrations, and report poor
performance with pure RL based training *REF*. Our
idea is to leverage the agent&apos;s policy for intelligent exploration to
jumpstart policy learning for the new task without human
demonstrations.


We reward the agent (+10) for every subgoal it achieves towards the
final task (e.g., for HEAT, these are &quot;put object on stove&quot;, and
&quot;turn-on burner&quot;). We fine-tune for 500k frames using PPO, and measure
success rate over 400 randomized episodes from the same environments.
The results in Fig. *REF* (left) show the benefit of the
proposed pretraining. Agents trained to be curious or cover more area
(CURIOSITY and NOVELTY) are not equipped to seek out useful
environment interactions, and suffer due to sparse rewards.
OBJCOVERAGE benefits from being trained to visit objects, but falls
short of our method, which strives for novel interactions. Our method
outperforms others by large margins across all tasks, and it learns
much faster than the best baseline (Fig. *REF*, right).


Conclusion


We proposed the task of &quot;interaction exploration&quot; and developed agents
that can learn to efficiently act in new environments to prepare for
downstream interaction tasks, while simultaneously building an
internal model of object affordances. Future work could model more
environment state in affordance prediction (e.g., what the agent is
holding, or past interactions), and incorporate more complex policy
architectures with spatial memory. This line of work is valuable for
increasingly autonomous robots that can master new human-centric
environments and provide assistance.