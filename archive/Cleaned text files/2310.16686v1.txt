Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies


Introduction


Reinforcement learning (RL) is a powerful tool, and has displayed recent success in several settings *REF* -- *REF*. However, despite its potential, RL faces a significant challenge: generalisation. Current agents and algorithms struggle to perform well beyond the narrow settings in which they were trained *REF* -- *REF*, which is a major limitation that hinders the practical application of RL. For RL algorithms to be effective in real-world scenarios, they must be capable of adapting to changes in the environment and performing well in settings that are different, but related, to those they trained on *REF*.


To illustrate this point, consider a legged robot trained to walk on a tiled floor. While the agent would perform well in these conditions, it will likely struggle in other reasonable settings, such as walking on asphalt or when carrying cargo. When walking on another surface, the friction characteristics would differ from what the robot trained on, which may cause it to slip. When carrying additional weight, the robot is heavier, with a potentially different center of mass. This may mean that the robot must exert more force to lift its legs or be more careful to prevent falling over. This limitation is far from ideal; we want our agents to excel in tasks with minor differences in dynamics (the effect of the agent&apos;s actions) without requiring additional training.


Several methods have been proposed to address this problem, including training a single agent in various settings in the hope that it learns a generalisable and robust behaviour *REF*, *REF* -- *REF*. However, this approach is not without its drawbacks, as it fails when variations are too large for a single behaviour to perform well in all settings *REF*, *REF*. For example, using the same behaviour when walking unencumbered and under load may lead to suboptimal performance in one or both cases.


This observation has led to other methods that instead use contextual information to choose actions, allowing the agent to adapt its behaviour to the setting it is in. For instance, if the agent knows when it is walking unencumbered vs. carrying something heavy, it can perform slightly differently in each case *REF*. The context differs conceptually from the state information, which generally includes data that the robot observes using its sensors, such as its joint angles, acceleration, and camera views. The crucial distinction between context and state is that context changes at a longer timescale compared to state; for instance, the floor friction and additional mass remain the same for a long time, whereas the robot&apos;s joint angles change after every action *REF*.
Currently, the predominant approach is to use context as part of the state information, ignoring the conceptual difference between these two aspects *REF* -- *REF*. However, this may lead to the agent confounding the context and state, and thus exhibit worse generalisation *REF*. This problem can be exacerbated in real-world settings, where identifying which variables affect the dynamics may be challenging *REF*. This may lead to several irrelevant variables being added to the context, further expanding the state space and making learning more difficult.


To address this problem, we introduce an approach to incorporating context into RL that leads to improved generalisation performance. Our method separates the context and state, allowing the agent to decide how to process the state information based on the context, thus adapting its behaviour to the setting it is in. Our experimental results demonstrate the effectiveness of our approach in improving generalisation compared to other methods. We show that our approach outperforms (1) not incorporating context information at all; (2) simply concatenating context and state; and (3) competitive baselines.
We also demonstrate that our approach is more robust to irrelevant distractor variables than the concatenation-based approach across multiple domains. We further provide a theoretical characterisation of problems where incorporating context is necessary and empirically demonstrate that a context-unaware method performs poorly. 


Background


Reinforcement learning problems are frequently modelled using a Markov Decision Process (MDP) *REF*, *REF*. An MDP is defined by a tuple ⟨S, A, T, R, γ⟩, where S is the set of states, A is the set of actions. *MATH* is the transition function, where *MATH* specifies the probability of ending up in a certain state s′ after starting in another state s and performing a specific action a. *MATH* is the reward function, where *MATH* specifies the reward obtained from executing action at in a state st and γ ∈ [0, 1] is the environment discount factor, specifying how short-term and long-term rewards should be weighted. The goal in reinforcement learning is to find a policy *MATH* that maximises the return *MATH*  *REF*.


We consider the Contextual Markov Decision Process (CMDP) *REF* formalism, which is defined by a tuple ⟨C, S, A, M′, γ⟩, where C is the context space, S and A are the state and action spaces respectively, and M′ is a function that maps a context c ∈ C to an MDP M = ⟨S, A, T c, Rc, γ⟩. A CMDP thus defines a family of MDPs, that all share an action and state space, but the transition (T c) and reward (Rc) functions differ depending on the context. Our goal is to train on a particular set of contexts, such that we can generalise well to another set of evaluation contexts. We focus solely on generalising over changing dynamics and therefore fix the reward function, i.e., *MATH*.


Related Work


Generalisation in RL is a widely studied problem, with one broad class of approaches focusing on robustness. Here, a single policy is trained to be robust to changes that may occur during testing, i.e., to perform well without failing catastrophically *REF*, *REF*, *REF*, *REF* -- *REF*. Many of these approaches are successful in this goal and can generalise well when faced with small perturbations. However, using one policy for multiple environments (or contexts) may lead to this policy being conservative, as it cannot exploit the specifics of the current environment *REF*. Furthermore, these policies are limited to performing the same action when faced with the same state, regardless of the setting. This may be suboptimal, especially when the gap between the different settings widens *REF*, *REF*.


This observation motivates the next class of methods, context-adaptive approaches. These techniques often use a version of the CMDP *REF* formalism, and use the context to inform the agent&apos;s choice of action. This allows agents to exhibit different behaviours in different settings, which may be necessary to achieve optimality or generalisation when faced with large variations *REF*. There are several ways to obtain this context, such as assuming the ground truth context *REF*, using supervised learning to approximate it during evaluation *REF*, *REF*, or learning some unsupervised representation of the problem and inferring a latent context from a sequence of environment observations *REF*, *REF* -- *REF*.


Despite the progress in inferring context, how best to incorporate context has received less attention. The most prevalent approach is to simply concatenate the context to the state, and use this directly as input to the policy *REF* -- *REF*, *REF* -- *REF*, which is trained using methods such as Soft Actor-Critic *REF* or Proximal Policy Optimisation *REF*. This ignores the conceptual difference between the state and context, and may lead to worse generalisation *REF*. This approach may also be limited when there are many more context features compared to state features *REF*. While concatenation is the most common, some other approaches have also been proposed. For instance, Biedenkapp et al. *REF* learn separate representations for context and state, and then concatenate the representations near the end of the network. They also use another baseline that concatenates a static embedding of the context features to the state. Both of these techniques generalise better than the concatenation approach. Another method, FLAP *REF*, aims to improve generalisation by learning a shared representation across tasks, which is then processed by a task-specific adapter in the form of a linear layer. Given state s, the policy network outputs features ϕ(s) ∈ Rd, which are shared across tasks. The final action is chosen according to Wiϕ(s) + bi, with a unique weight matrix Wi and bias vector bi which are learned separately for each task. Concurrently, a supervised learning model is trained to map between transition tuples (st, at, rt+1, st+1) and these learned weights. At test time, this supervised model generates the weights, while the policy network remains fixed.
FLAP generalises better to out-of-distribution tasks compared to other meta-learning approaches *REF*, *REF*, *REF*. However, this approach requires a separate head for each task, which may scale poorly if we have many tasks or see each task only a few times.


One promising approach that has been used in other fields is Feature-wise Linear Modulation (FiLM) *REF*, *REF*. In this method, features in one modality (e.g., natural language text) linearly modulate the neural-network features obtained from another modality (e.g., visual images). An example of this would be processing a visual scene, modulated by different natural language questions which would lead to a different final answer. In RL, Benjamins et al. *REF* introduce cGate, *REF* which follows a similar procedure to FiLM; in particular, the neural network policy receives the state as input and outputs an action. Before the final layer, however, the context is first transformed by a separate network, and then used to modulate the state-based features. This approach showed promise, but restricted the context features&apos; effect on the state-based features to be linear.


Another research area that has garnered more attention recently is learning general foundation models for control *REF* -- *REF*. Inspired by recent work in fields such as natural language processing *REF* and computer vision *REF*, these models aim to provide a general foundation that can be easily fine-tuned for particular tasks of interest. For instance, Gupta et al. *REF* learns a general controller for several different robot morphologies. The robot&apos;s configuration is encoded and passed to a transformer *REF*, allowing one model to control a wide variety of morphologies. While these works directly train policies, Schubert et al. *REF* instead train a dynamics model, and use a standard planning technique -- model-predictive control *REF* -- *REF* -- to choose actions. They find that this approach achieves better zero-shot generalisation compared to directly learning a policy. On a different note, Sun et al. *REF* pre-train a foundation model that predicts observations and actions. Then, during fine-tuning, they train a policy to perform a particular task using the pre-trained representations, which enables the use of either imitation learning or reinforcement learning in the downstream fine-tuning stage.


Finally, much work in meta-reinforcement learning *REF* relates to the problem of generalisation.
Meta-learning approaches generally aim to use multiple tasks during training to learn how to learn; this knowledge can then be used during testing to rapidly adapt to a new task *REF*, *REF* -- *REF*. In this problem setting, Beck et al. *REF* use a recurrent model to encode the current task, and generate the weights of the agent policy based on the task encoding. Sarafian et al. *REF* take a similar approach, but instead generate the weights using the environment state, and processing the encoded context using the generated network. Another approach, MAML *REF* aims to learn the weights of a neural network such that, for any new task, performing only a few gradient updates would lead to a high-performing task-specific network. While meta-learning can lead to highly adaptable agents, many methods require multiple episodes of experience in the target domain *REF*, *REF*, and may therefore be less well-suited to the zero-shot setting we consider *REF*.


Theoretical Intuitions


Now, while only a context-conditioned policy is guaranteed to be optimal on a general CMDP *REF*, in many cases simply training an unaware policy on a large number of diverse environments can lead to impressive empirical generalisation *REF*.
In this section, we characterise and unify these two observations. In particular, for a specific problem setting -- where the context defines the target location an agent must travel to -- we find that:


(i) For some context sets, an unaware policy will perform arbitrarily poorly on average, due to it being forced to take the same action in the same state, regardless of context.


(ii) However, for other context sets -- where the different contexts are similar enough -- an unaware policy can perform well on average, as it can simultaneously solve each task.


We defer the formal statement of this theorem and its proof to Appendix [A], but here we briefly provide some intuition. Overall, our results rely on the fact that a context-unaware agent must perform the same action in the same state, regardless of context. Therefore, in a setting where we have different goals (indicated by the context), and making progress on one goal leads to making negative progress on another, a context-unaware agent cannot simultaneously perform well on both of these contexts. By contrast, if the goals are aligned in the sense that making progress on one leads to progress on another, a context-unaware agent can perform well on average. A context-aware agent, however, can always choose the correct action in either scenario.


FIGURE 1


Fig. [1] visually illustrates these two cases and summarises the implications. In Fig. [1a], if there is no overlap between goals, an unaware policy is forced to visit each one in sequence. This is because it must perform the same action in the same state, regardless of the context, meaning that it cannot always travel to the correct goal. This policy will therefore have a low value function compared to the optimal context-aware policy. By contrast, in Fig. [1b], a single unaware policy can directly go to the intersection of all the goals, leading to only slightly less return than the optimal context-aware policy. While the formal result in Appendix [A] merely shows the existence of some problems with these properties, we believe the concept generalises to more complex settings. In particular, if the problem exhibits some structure where the same policy can simultaneously make progress on each context, an unaware model can perform well. However, if making progress on one goal leads to the agent moving away from another, a context-unaware policy can at best perform arbitrarily worse than a context-aware one, and incorporating context is crucial. We empirically substantiate these findings in Section [7] and formally consider this example in Appendix [A]. Finally, see Appendix [B] for more details about how these theoretical results connect back to our empirical observations.


The Decision Adapter


An adapter is a (usually small) neural network with the same input and output dimensions. This network can be inserted between the layers of an existing primary network to change its behaviour in some way. These modules have seen much use in natural language processing (NLP) *REF* -- *REF* and computer vision, usually for parameter-efficient adaptation *REF* -- *REF*. In these settings, one adapter is usually trained per task, and the base model remains constant across tasks. Our method is inspired by the use of adapters in these fields, with a few core differences.


First, since our aim is zero-shot generalisation, having a separate adapter module for each task (corresponding to context in our case) provides no clear means for generalisation. Relatedly, since tasks are generally discrete in NLP, but we allow for continuous contexts, this approach would be problematic. Furthermore, the number of learnable parameters increases with the number of training contexts, presenting a computational limitation to the scalability. To circumvent these problems, we use a hypernetwork *REF* to generate the weights of the adapter module based on the context. This allows the agent to modify its behaviour for each context by having a shared feature-extractor network which is modulated by the context-aware adapter. Moreover, since the hypernetwork is shared, experience gained in one context could transfer to another *REF*.
Second, instead of having separate pre-training and adaptation phases as is common in NLP *REF*, we do not alter the standard RL training procedure at all. We change only the architecture and train normally -- allowing for the easy integration of our approach into standard RL libraries. Finally, our method is agnostic to the exact RL algorithm and can be applied to most off-the-shelf algorithms without much additional effort.


FIGURE 2


To formally describe our approach (illustrated in Fig.
[2]), suppose we have a state s ∈ S, context c ∈ C, and a standard n-layer fully-connected neural network architecture that predicts our action a. Here  *MATH*, *MATH*. We add an adapter module Ai between layers Li−1 and Li. This adapter module is a multilayer neural network  *MATH*  consisting of parameters  *MATH*. *REF* These parameters are generated by the corresponding context-conditioned hypernetwork  *MATH*  (line 5 in Algorithm [1]) and reshaped to form multiple weight matrices. Then, before we process xi using layer Li, we pass xi through the adapter module Ai (line 6) defined by the generated weights and biases to obtain  *MATH*. Furthermore, akin to the application of adapters in other fields *REF*, *REF*, we add a skip connection, by setting *MATH* (line 7). This updated xi is then the input to layer Li (line 9). Additionally, we could have multiple adapter modules in the primary network, with at most one module between any two consecutive layers (lines 3 and 4).


Finally, we note that the Decision Adapter model can learn to implement the same function as cGate *REF*, but is theoretically more powerful. While cGate uses a linear elementwise product between the context and state features, our model uses a hypernetwork to generate the weights of a nonlinear adapter module.
Our hypernetwork is general enough to be able to recover this elementwise product, but is not constrained to do so. See Appendix [C] for a more formal treatment of this point.


Experimental Setup


Metrics


Our evaluation strategy is as follows: We take the trained model, and compute the average of the total episode reward over n = 5 episodes on each evaluation context c to obtain a collection of contexts C and corresponding rewards R. We then calculate the context c. This metric is high when the agent performs well across a wide range of contexts, and thus corresponds to generalisation performance. However, since we generally have Ctrain ⊂ Cevaluation, this metric also incorporates how well the agent performs on the training contexts. Despite this, it is still a useful metric for generalisation, as our training context set typically contains only a handful of contexts, with most of the evaluation contexts being unseen.


Baselines


We use Soft-Actor-Critic *REF* for all methods, to isolate and fairly compare the network architectures. We aim to have an equal number of learnable parameters for each method and adjust the number of hidden nodes to achieve this. We use the following baselines, with more details in Appendix [D]:


Unaware This model simply ignores the contextual information and just uses the state. This approach allows us to evaluate how well a method that does not incorporate context performs.


Concat This method forms a new, augmented state space *MATH*, and then gives the model the concatenation [s; c] of the state and context *REF* -- *REF*, *REF*. This baseline allows us to compare our method to the current standard approach of using contextual information.


cGate cGate *REF* learns separate state (ϕ(s)) and context (g(c)) encoders, and predicts the action *MATH*, where f is the learned policy and ⊙ is the elementwise product.


FLAP FLAP *REF* learns a shared state representation across tasks, which is then processed by a task-specific linear layer that is generated by conditioning on the context.


Our Adapter configuration is more fully described in Appendix [D.3], and Appendix [E] contains ablation experiments comparing the performance of different hyperparameter settings.


Environments


1. ODE


This environment is described by an ordinary differential equation (ODE), parametrised by n variables making up the context. The dynamics equation is *MATH*, with *MATH*, truncated at some cn−1. The episode terminates after 200 timesteps, with the reward function: *MATH*.


This incentivises the agent to give control a to keep the state close to x = 0. The context in this environment is *MATH*. The action space is two-dimensional, continuous and bounded, i.e., *MATH*. The action is interpreted as a complex number, a = a0 + a1i, to allow the dynamics equation to be solvable, even in normally unsolvable cases such as *MATH*. Here, restricting a to R would result in x˙ ≥ 0 and an unsolvable system for initial conditions x0 &gt; 0. We keep only the real part of the updated state and clip it to always fall between −20 and 20.


The ODE acts as a conceptually simple environment where we can arbitrarily scale both the number (i.e., n) and magnitude (i.e., |ci|) of the contexts and precisely measure the effects.
Furthermore, since many dynamical systems can be modelled using differential equations *REF* -- *REF*, the ODE can be considered a distilled version of these. Finally, context is necessary to perform well in this environment, making it a good benchmark. See Appendix [D] for more details.


CartPole


CartPole *REF* is a task where an agent must control the movement of a cart to balance a pole vertically placed upon it.
The observation space is a 4-dimensional real vector containing the cart&apos;s position x and velocity x˙, as well as the pole&apos;s angle θ and angular velocity θ˙. We use a continuous action space where a ∈ [−1, 1] corresponds to the force applied to the cart (where negative values push the cart to the left and positive values to the right). The reward function is +1 for each step that the pole is upright. An episode terminates when the pole is tilted too far off-center, the cart&apos;s position is outside the allowable bounds or the number of timesteps is greater than 500. We follow prior work and consider the variables Gravity, Cart Mass, Pole Length, Pole Mass and Force Magnitude collectively as the context *REF*, even if only a subset of variables change. Here, we change only the pole length and evaluate how susceptible each model is to distractor variables. In particular, during training, we also add k additional dimensions to the context, each with a constant value of 1. Then, during evaluation, we set these values to 0. This may occur during a real-world example, where accurately identifying or inferring the pertinent context variables may be challenging *REF*, *REF*, and lead to several irrelevant variables.
Overall, this means that the context is represented as a 5 + k- dimensional vector. This environment is useful as it (1) is a simple setting with models that can be trained quickly, and (2) still has desirable aspects, namely a way to perturb some underlying variables to change the dynamics of the environment. Furthermore, prior work has shown that even standard, unaware RL algorithms can generalise well in this environment *REF* (in the absence of distractor context variables), which allows us to investigate the benefits of context and detriment of distractors.


Mujoco Ant


As a more complex and high-dimensional problem, we consider Ant from the Mujoco suite of environments *REF*, *REF*. Here, the task is to control a 4-legged robot such that it walks. The observation space consists of the robot&apos;s joint angles and velocities, as well as contact forces and torques applied to each link, totalling 111 dimensions. The action space A = [−1, 1] represents the torque applied to each of the 8 joints.
Furthermore, each episode is terminated after 1000 timesteps, with a positive reward for not having fallen over at each step and moving forward. The reward function also penalises large control forces.
Here, the mass is the context variable, which is 5 by default. We train on the set {5, 35, 75} and evaluate on 200 evenly spaced points between 0.5 and 100.


Results


Generalisation Performance


In this section, we consider the ODE environment, with a training context set of {−5, −1, 1, 5} and 300k training timesteps. To measure generalisation, we use an evaluation context range consisting of 201 equally spaced points between −10 and 10 (inclusive). In the left pane of Fig. [3], we plot the average performance across the entire evaluation range (as discussed in Section [6.1]) at different points during training.


Firstly, we can see that the Unaware model fails to generalise well, since the optimal actions in the same state for two different contexts may be completely different. This result substantiates our theoretical analysis in Section [4], highlighting the importance of using context in this domain. Concat, cGate and FLAP perform reasonably well, but our Decision Adapter outperforms all methods and converges rapidly. In Appendix [F], we plot separate subsets of the evaluation range: Train, Interpolation and Extrapolation, corresponding to the contexts in the training set, the contexts within the region [−5, 5] and the contexts outside the convex hull of the training contexts, containing c ∈ [−10, −5] ∪ [5, 10], respectively. There we find that most methods, with the exception of the Unaware model, perform well on the training contexts, and the Adapter outperforms all methods when extrapolating. These results also extend to the multidimensional context case, shown in Fig. [3] (right). We train on the context set given by {1, 0, −1}2 ∪ {5, 0, −5}2, with the context (0, 0) being omitted due to it being unsolvable. The evaluation range consists of the cartesian product A2, where A contains 21 equally-spaced points between −10 and 10. The Adapter outperforms all methods, with the Concat model coming second. FLAP and the Unaware models struggle in this case.


FIGURE 3


Robustness to Distractors


For this and the next section, we consider the Concat and cGate models as baselines. The reasons for this are that (1) the Concat model performed comparably to or better than the other baselines in the previous sections, making it a representative example of the context-aware approaches; and (2) the Concat model is currently a very prevalent way of incorporating context *REF* -- *REF*, *REF* -- *REF*; and (3) cGate is the closest approach to our work in the current literature.


The results for CartPole are shown in Fig. [4]. When there are no distractor variables, the Adapter, Concat and cGate models perform comparably. In this case, each architecture achieves near the maximum possible reward for the domain. Further, we see little effect after adding just one confounding context variable.
However, as we add additional distractor variables, the Concat and cGate models&apos; performances drop significantly, whereas the Adapter&apos;s performance remains relatively stable. Strikingly, the Adapter architecture trained with 100 distractor context variables is still able to perform well on this domain and significantly outperforms the Concat model with significantly fewer (just 20) distractor variables.
This demonstrates that, given only useful context, concatenating state and context is a reasonable approach in some environments. However, this is a strong assumption in many practical cases where we may be uncertain about which context variables are necessary. In such cases, the consequences of ignoring the conceptual differences between state and context are catastrophic and it is necessary to use the Adapter architecture. We find a similar result in the ODE, which we show in Appendix [F.5].


As an extension to the previous experiment, here we consider using non-fixed distractor variables, i.e., sampling them from Gaussian distribution with different means. The intuition behind this experiment is similar to before, but allows for small variations within training and testing. In particular, we use σ = 0.2 and a mean of either 0 or 1. In this experiment, a new distractor context is sampled at the start of every episode. During training this is sampled from N(1, 0.2) and during evaluation we sample from N(0, 0.2). In CartPole, the results in Fig. [5] show that the Adapter is still more robust to changing distractor variables compared to either Concat or cGate. See Appendix [F.6] for more details, results on the ODE domain, and additional experiments when the mean of the Gaussian does not change between training and testing.


FIGURE 5


FIGURE 6


High-Dimensional Control


We next consider a more complex and challenging continuous-control environment, that of the Mujoco Ant. We run the same experiment as in CartPole, where we add distractor variables, with no effect on the dynamics, and different values between training and testing. These results are shown in Fig. [6]. Overall, we observe a similar result to that of CartPole -- the Concat and cGate models are significantly less robust to having irrelevant distractor variables.
By contrast, the Adapter consistently performs well regardless of how many distractor dimensions are added.


Limitations


While our Adapter model performs well empirically, it does have some limitations, which we summarise here and expand on in Appendix [G]. First, we find that using an incorrect and noisy context (Appendix [G.1]) generally resulted in the Adapter model performing worse, particularly as we increased the level of noise. However, the Adapter model still performs comparably to the Concat model in this case. Second, we generally normalise the contexts before we pass them to the models, relative to the maximum context encountered during training. While our Adapter performs well when the context normalisation is incorrect by a factor of 2 or 3, its performance does suffer when the normalisation value is orders of magnitudes too small, leading to very large contexts being input into the model (see Appendix [G.2] for further discussion). Third, when training on a context range that is too narrow (see Appendix [G.3]) or does not exhibit sufficient variation, our model is susceptible to overfitting, which leads to worse generalisation. Even if our model does not overfit, an overly narrow context range would also lead to poor generalisation.
Thus, ensuring that agents are trained in sufficiently diverse environments is still important. Fourth, while we assumed access to the ground-truth context, it may not always be available in practice *REF*. Therefore, integrating the Decision Adapter into existing context inference methods *REF*, *REF* is a promising avenue for future work. A final limitation of our Adapter is its longer wall-clock training time compared to other methods (given the same training timesteps and a similar number of parameters). This is likely caused by the hypernetwork that we use. However, we did not utilise strategies such as caching the generated weights for a particular context (which would be possible since context remains constant for an episode), or more effectively batching the forward passes through the Adapter which could ameliorate this issue in practice.


FIGURE 6


Beyond addressing these limitations, there are several logical directions for future work. First, we only consider state and context features to be numerical vectors; therefore, extending our approach to different modalities -- for instance, image observations and natural language context -- would be a natural extension. In principle, our Adapter model could be directly applied in this setting without any fundamental modifications. One option would be to explore adding the hypernetwork directly into the modality-specific model (e.g., a convolutional neural network in the case of images), and the alternative would be to integrate the adapter module in the final (fully-connected) layers of the RL policy. Second, while we focused on the problem of zero-shot generalisation, our approach may also be suitable for efficient few-shot learning. In particular, adapters in NLP are often added to a large pre-trained model and then fine-tuned while freezing the primary model *REF*. Using the Decision Adapter in this paradigm is a promising direction for future work, especially in cases where the difference between training and testing is so large that we cannot expect effective zero-shot generalisation. Combining our work with fine-tuning-based *REF* -- *REF* or meta-learning *REF*, *REF*, *REF*, *REF*, *REF* -- *REF* approaches, which generally focus more on this case, could likewise be promising.


Conclusion


In this work, we consider the problem of generalising to new transition dynamics. We first illustrate that for some classes of problems, a context-unaware policy cannot perform well over the entire problem space -- necessitating the use of context. We next turned our attention to how context should best be incorporated, introducing the Decision Adapter -- a method that generates the weights of an adapter module based on the context. When context is necessary, our Decision Adapter outperforms all of our baselines, including the prevalent Concat model. Next, when there are some irrelevant context variables that change between training and testing, the Concat and cGate models fail to generalise, whereas our Decision Adapter performs well. This result holds in several domains, including the more complex Ant.
Furthermore, our Adapter is theoretically more powerful and empirically more effective than cGate, showing that our hypernetwork-based perspective is useful when dealing with context.
Ultimately, we find that the Decision Adapter is a natural, and robust, approach to reliably incorporate context into RL, in a factored way. This appears to be of particular benefit when there are several irrelevant context variables, for instance, in real-world settings *REF*.
