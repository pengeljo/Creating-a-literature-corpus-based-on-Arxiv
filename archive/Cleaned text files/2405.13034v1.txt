Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality


Introduction


IMAGE 


FIGURE


The advent of &quot;Industry 4.0&quot;, centered on the concept of smart
manufacturing, presents a landscape with both opportunities and
challenges for enhancing production efficiency (*REF*, *REF*; *REF*, *REF*; *REF*, *REF*). Training assistance for
automating and accelerating industrial assembly is in huge demand
across various manufacturing applications, such as furniture manufacturing (*REF*, *REF*),
industrial product assembly (*REF*, *REF*), and car assembly (*REF* *REF*, *REF*).


Mixed reality (MR), encompassing both virtual reality (VR) and
augmented reality (AR), spans a spectrum from fully real environments
to &quot;matrixlike&quot; virtual environments, showing promise for industrial
manufacturing assembly tasks (*REF* *REF*, *REF*; *REF*, *REF*; *REF*, *REF*). These multimodal,
interactive, user-centric environments provide a solution for trainees
who experience significant cognitive workload for training (*REF*, *REF*; *REF*, *REF*; *REF* *REF*, *REF*). However, the assistance of
a senior person as a trainer is required, either in person or remotely (*REF*, *REF*).


FIGURE


To advance intelligent virtual assistants, traditional work
leverages natural language processing (NLP) techniques (*REF*, *REF*; *REF*,
*REF*, *REF*; *REF*, *REF*) and reinforcement learning
(*REF*, *REF*) to promote human-machine interactions. LLMs, as the new era of prevalent NLP
techniques, have been observed to elicit diverse interaction patterns
across tasks, demonstrating its versatility and feasibility
(*REF* *REF*, *REF*). However, (i) tailoring assistant services by
grounding interactions; and (ii) understanding users&apos; situated
multimodal contexts remain challenging and under-explored (*REF*, *REF*).


To this end, we introduce an autonomous workflow (see Figure
*REF*) tailored for seamlessly integrating AI agents into
MR applications for fine-grained training. We present a demonstration
of a multimodal fine-grained training assistant within a toy MR
application for LEGO brick assembly. Specifically, we design a
cerebral language agent that integrates LLM with memory, planning, and
interaction with MR serving functional tools and a vision-language
agent, enabling agents to decide their actions based on experiences.
Then, we introduce LEGO-MRTA, a multimodal fine-grained assembly
dataset synthesized automatically by a commercial LLM. This dataset
comprises 65 multimodal instruction manuals, 1,423 conversations
with vision question answering, serving usages of 18 functional tools in an MR environment. 
Additionally, several prevailing open-resource LLMs are presented as benchmarks, assessing
their performance with and without fine-tuning on the proposed
dataset. Furthermore, we anticipate that the broader impact of this
workflow will advance the development of smarter assistants for
seamless user interaction in MR environments, fostering research in
both AI and HCI communities.


We summarize our contributions as follows:
- We design a workflow, which seamlessly integrates autonomous AI
agents for fine-grained assembly assistance in an MR demonstration.
- We create a multimodal manual-grounded finegrained assembly
conversation dataset in the MR context.
- We assess several open-resource LLMs as benchmarks, evaluating
their performance with and without fine-tuning on the proposed
dataset.


Related Work


For thoroughness, we provide preliminaries concerning multimodal
datasets and virtual dialogue assistants within the realm of mixed
reality (MR).


Multimodal Datasets towards MR


Traditional multimodal datasets focus on the interactions with
sensor data (*REF*, *REF*) between human-human or human-robot, and only a few of them provide
small-scale task-oriented dialogues, such as OFAI-MMTD (*REF* *REF*, *REF*) and
(*REF*, *REF*), and Chinese Whispers (*REF*, *REF*). ScanScribe (*REF*,
*REF*) releases a 3D scene-text pairs dataset for 3D
vision and text alignment learning. HoloAssistant (*REF*, *REF*) provides a dataset
containing 350 unique instructor-performer pairs with AR metadata to
perceive, reason, and interact in the physical world. However, the
conversations are not publicly available.


TABLE


Recent studies have concentrated on multimodal datasets with
conversations. MDC (*REF* *REF*, *REF*) presents a collection of 509
humanhuman conversations in the Minecraft VR games. CerealBar (*REF*, *REF*) creates 1,202 human-to-human 
conversations that map user instructions to system actions in
a situated VR game environment. CVDN (*REF*, *REF*) collects 2,050 human-robot
conversations on Amazon Mechanical Turk for improving parsing and
perception for natural language commands. Teach
(*REF* *REF*, *REF*) builds over 3,000 human--human, interactive
dialogues to complete household tasks in the simulation.


Different from those aforementioned datasets, LEGO-MRTA gathers 1,423
human-human natural conversations between trainers and trainees. Unlike robotic commands, the length of utterances is relatively longer.
Furthermore, these conversations are generated by grounding both on an
instruction manual and responses from an MR, ensuring that the
simulated conversations closely resemble natural human language. We
compare the statistics of the above datasets in Table *REF*.


Virtual Dialogue Assistants for MR


Conventional efforts focus on creating virtual assistants for
human-machine interactions using NLP techniques (*REF*, *REF*; *REF*,
*REF*, *REF*; *REF*, *REF*) and reinforcement learning
(*REF*, *REF*). LLMs, representing the forefront of contemporary NLP techniques, hold tremendous
promise for advancing towards the next generation of intelligent
assistants (*REF* *REF*, *REF*). The recent remarkable achievements of LLMs
have spurred a growing interest in utilizing them to address a variety of complex tasks (*REF*, *REF*), 
with particular attention being drawn to LLM-augmented autonomous agents 
(*REF*, *REF*; *REF*, *REF*; *REF* *REF*, *REF*; *REF*, *REF*).


Autonomous agents expand the capabilities of LLMs into sequential
action execution, demonstrating their proficiency in interacting
with environments and addressing complex tasks through data
collection (*REF*, *REF*; *REF*, *REF*). A crucial aspect of this
advancement relies on the capacity of LLMs to generate and interpret
images, enabling them to access visual content and provide inputs,
thereby integrating with mixed reality (MR) environments (*REF*, *REF*; *REF* *REF*, *REF*). 
Regarding skill training, autonomous agents and LLMs can create immersive learning experiences
that blend virtual and physical environments. For instance, students
can utilize them to explore workflows and concepts in a more interactive and engaging manner (*REF*,
*REF*; *REF* *REF*, *REF*). In the context of MR serving as a sandbox
(*REF*, *REF*) for LLMs and autonomous agents, the relationship is mutually beneficial. MR offers
a secure (*REF*, *REF*), adaptable, and regulated setting for training models. Together, LLMs,
autonomous agents, and MR hold the potential to revolutionize our
interaction with the digital world (*REF*, *REF*).


The convergence of LLMs, autonomous agents, and MR presents both
excitement and challenges. As MR training experiences become more
realistic and personalized, they demand greater amounts of data,
encompassing detailed information about trainees&apos; behaviors,
preferences, and interactions. Ensuring the availability and
reusability of this data poses a significant challenge. Overall, our
workflow aims to enhance MR training experiences by facilitating
more natural language interactions, generating precise 3D models of
real-world objects (*REF*, *REF*, *REF*), and fostering dynamic and interactive
experiences. While challenges remain (*REF* *REF*, *REF*; *REF*, *REF*), the potential of this
powerful technological fusion offers numerous exciting possibilities that could revolutionize personalization in
virtual experiences. This entails the development of dedicated
workflows and datasets.


Fine-grained Training Workflow


In this section, we describe the proposed workflow (See Figure
*REF*) that advances AI agents towards MR guided
fine-grained training.


Definition of Fine-Grained Training


In the context of fine-grained training, we anticipate the ability
to (i) accurately follow professional training instructions documented
in an instruction manual and; (ii) be sensitive to detailed visual
information, ultimately for complex industrial assembly tasks, as
illustrated in Figure *REF* (a).


We define the following two roles during a training session:
- User: A human trainee who aims to acquire expertise and will
work on fine-grained assembly tasks through interaction with the MR
environment
- Assistant: A virtual AI agent who will be able to assist the
trainees in training and respond to their inquiries. It offers
support with (i) a conversation agent that replies to trainees&apos;
requests and provides guidance grounded in the instruction
manual; (ii) an interface for users to interact with MR environment;
and (iii) a vision-language agent that understands and transmits
users&apos; visual context to language.


Autonomous AI Agent


We design the autonomous AI agent with a chain of two agents, namely
(i) a cerebral language agent that serves to reply to trainees&apos;
requests, provide guidance, interact with MR and the vision-language
agent; and (ii) a vision-language agent that understands and transmits
users&apos; visual context to language, which is then utilized by the
cerebral language agent for planning.


Cerebral Language Agent


Inspired by the concept of LLM-powered autonomous agents (*REF*, *REF*), we develop a cerebral
language agent that incorporates an LLM with memory, planning, and
functional tools that can interact with MR application, thereby
enabling agents to make decisions regarding their actions based on
past experiences. It can handle multimodal inputs, such as
instruction manuals, historical conversations, and metadata within
MR environments, and subsequently generate actions (i.e., responses or API
calls for the MR application). The scope of responsibility of the
agent is defined in a system prompt (See P2, Table *REF*,
Appendix [A]) Notably, it is able to alleviate the
challenges (See ยง[1]): (i) it tailors assistant services by seamlessly
interaction with MR applications to discover the business needs
gradually; (ii) it interacts with a vision-language agent (See
ยง[3.2.2]), which facilitates the capability of
understanding the multimodal context in MR environments.


Vision-Language Agent


The vision-language agent&apos;s mission is to bridge the gap between
understanding visual context and language, enabling effective
utilization by the cerebral language agent (See
ยง[3.2.1]) to conduct comprehensive
planning for global optimization. Its core is the vision-language
model (VLM) which is a task-driven large model that transmits vision
input into language output needed by specific tasks. In the context of
LEGO assembly training, we observe two distinct patterns in LEGO
instruction manuals (See an example in Figure *REF*) and
define the following two tasks: (T1) Object detection. Given an
image or a sequence of images as input, the objective is to predict
the position of an object requested in a query and generate output
in the format of &quot;\&lt;Object\&gt; \&lt;Xleft\&gt; \&lt;Ytop\&gt; \&lt;Xright\&gt;
\&lt;Ybottom\&gt;&quot;. For example, during assembly step 2, the AI trainer
might direct the trainee, &quot;Please gather the earth blue pair of legs
and the silver metallic upper part of the body.&quot; In response, the
trainee may ask, &quot;Is this the one?&quot; The vision-language agent is
tasked with recognizing the object the trainee is referring to. (T2)
Assembly state detection. Given an image or a sequence of images
as input, the objective is to identify if the current assembly state
matches the reference state provided in the instruction manual. For
example, during assembly step 3, the vision-language agent is
responsible for assisting the user&apos;s request, such as &quot;Am I assembling
them correctly?&quot;


Pilot MR Application Design


We design an MR application as a pilot to show intuitive
demonstration. First, we utilized a commercial LLM to generate
candidate user requirements using the prompt (See P1, Table
*REF*, Appendix [A]) as input. Then, we
brainstormed and discussed the generated user requirements within a
group of researchers and developers and finalized 7 user
requirements (See Table *REF*, Appendix [A])
and 18 serving functional tools (See Table *REF*). We develop a standard application programming interfaces (APIs) to enable
seamless interactions between functions in MR application and AI
agent.


FIGURE


Dataset Creation


In this section, we introduce how to use the proposed workflow
(ยง[3]) to create a multimodal dialogue dataset in MR environment.


Instruction Manual Crawling


We crawled 65 multimodal instruction manuals for fine-grained training
from LEGO official website *REF*. A manual provides
illustrated images and textural instructions on how to use, operate,
assemble, and install a LEGO brick set. The key sections of an
instruction manual include: (i) a summary that
describes the general information, such as topics and candidate parts
for assembly. It is followed by (ii) a sequence of multimodal step
instructions. Each step contains a set of textual instructions and an
illustration by image. Key functional phrases such as theme entities
are highlighted in textual instructions. Here we show an example of a
LEGO instruction manual in Figure *REF*.


Tool Response Generation


First, we use the crawled instruction manuals and the well-designed
prompt template to produce prompts as an LLM input to generate user
functional requirements and decide the serving functional tools.
Then, we randomly choose up to 6 tools for each conversation session
and record the simulated responses generated from templates.


VLM-based QA Construction


First, we use the step instruction to construct a query, containing a
special token (&quot;\[detection\]&quot;) for the object detection task and a
single instruction in a step. Second, we employ a query and the
aligned image as inputs for MiniGPT-v2 (*REF*, *REF*; *REF*,
*REF*), generating inference output as an answer of
the query in the format of &quot;\&lt;Object\&gt; \&lt;Xleft\&gt; \&lt;Ytop\&gt; \&lt;Xright\&gt; \&lt;Ybottom\&gt;&quot;. 
Last, we iterate through all instruction steps in a conversation session, repeating the above
two steps to construct vision question answering (VQA) pairs.


Multimodal Context-Aware Conversation Generation


We generate conversations grounded on both the instruction manual and
tool responses using a commercial LLM. First, we reconstruct full
instruction manuals with a summary and 10 step instructions because
the average number of steps per manual is 215.3, which is quite long.
This may limit the input tokens of an LLM and potentially distract the
LLM with less grounding capability. Second, we instantiate the
designed prompt template (See P3, Table *REF*, Appendix
[A]) with the chunked instruction manuals. Last, we utilize
a commercial LLM as the core of the proposed workflow to generate the
conversations. Specifically, the system prompt informs the language
agent about its responsibilities. The query prompt is used for each
round of requests to generate a conversation. The historical rounds of
requests are tracked by memory.


TABLE


Dataset Statistics


We report the statistics of instruction manuals (See Table
*REF*) and conversations (See Table *REF*).


We obtain 65 instruction manuals as grounding to lead a commercial
LLM to generate 1,423 human-human natural conversations between trainers and trainees. 
Each instruction manual can make 28.3 on average. Theoretically, the amount of conversations can be
enlarged by multiple times of requests. However, we focus on
showcasing how to create meaningful datasets automatically. We
construct 26,405 context-response pairs from generated conversations
and VQA pairs as data samples. The average length is 107 tokens for
the context and 145 tokens for the response utterance. We utilize 21.1k samples for fine-tuning 
open-resource LLMs to enhance the instruction-following capability and evaluate their performance on
5.25k test samples. Compared with existing datasets, LEGO-MRTA ensures
that the simulated conversations closely resemble natural human
language because of the design of the simulation method.


Experimental Setup


LLM Benchmarks


TABLE


We consider several prevailing 7B open-source decoder-only LLMs as
benchmarks, considering privacy concerns associated with fine-grained
training in manufacturing.
- BLOOM (*REF*, *REF*) is
pretrained on the multilingual ROOTS corpus, offering multilingual capabilities
for various natural language processing tasks.
- Falcon-instruct (*REF*, *REF*) is pretrained on a large corpus of
RefinedWeb data and fine-tuned on mixed chat and instruct datasets.
- Llama2-Chat (*REF*, *REF*) is a pretrained and fine-tuned generative
text model optimized specifically for dialogue tasks, ensuring
high-quality conversational responses.
- Vicuna1.5 (*REF*, *REF*) is a chat assistant derived by fine-tuning Llama 2 on user-shared
conversations collected from ShareGPT.
- OpenChat3.5 (*REF*, *REF*) is a chat model fine-tuned with the C-RLFT strategy on mixed-quality
data, achieving performance comparable to larger models like
ChatGPT.
- XVERSE *REF* is a versatile model supporting 8k context length, ideal for longer multi-round dialogues, knowledge
question-answering, and summarization tasks, trained on a diverse
dataset of 2.6 trillion tokens.
- BlueLM-Chat *REF* is a large-scale language model
optimized for chat tasks, offering improved context understanding.
- Qwen-Chat (*REF*, *REF*) is
a chat model that fine-tunes the pretrained Qwen model using human
alignment techniques.
- Mistral-Instruct (*REF*, *REF*) is a finetuned version of the
Mistral-7B-v0.1, specifically tailored for instruction-based tasks
using publicly available conversation datasets.


TABLE


Evaluation Metrics


We evaluate the performance in terms of both overlap (BLUE-n, ROUGE-n)
and informativeness (ToolACC, ThemeACC):
- BLUE-n measures precision, which measures the ratio of n-grams
in the generated responses that match those in the reference
responses. We consider n = 4.
- ROUGE-n measures recall, which calculates the ratio of n-grams
in the reference responses that are captured by the generated
responses. Here we consider n = 1, 2, L and L denotes the
number of longest common subsequences.
- ToolACC is defined as the ratio of correctly mentioned
entities by the generated responses, compared to the reference
response, from a list of serving tools.
- ThemeACC is defined as the ratio of correctly response, 
from a list of theme entities obtained from the instruction manual.


Implementation Details


The implementation of the workflow is based on
Langchain. *REF* The &quot;gpt-3.5-turbo-16k-0613&quot; is used as
the commercial LLM for generating data, e.g., conversations, user
requirements, and serving functions. The
MiniGPT4-v2 *REF* is used as the VLM to detect the
object, followed by simple rules to generate VQAs.


We use LoRA to conduct PEFT 7B open-source LLMs with the proposed
dataset based on framework (*REF*, *REF*). Specifically, the maximum sequence length is
1024 and the learning rate is 5e-05. The model is trained for 3 epochs
with a per-device batch size of 4, and accumulated gradients every 4
steps. A cosine learning rate scheduler is employed, with a maximum
gradient norm of 1.0. We log results every 5 steps and save model checkpoints every 100
steps. Warm-up steps are set to 0. LoRA is used with a rank of 8 and a
dropout rate of 0.1 for regularization. All experiments are run on
NVIDIA A100 SXM4 40GB GPUs.


Outcomes


Evaluation on Benchmark LLMs


Table *REF* shows the performance on 9 prevailing
open-source LLMs, without and with fine-tuning on the LEGO-MRTA
dataset.


First, after fine-tuning, the performance of all models dramatically improved in terms of all metrics, except for the
results that are underlined. This mentioned entities compared to the reference redemonstrates the feasibility and effectiveness of tailoring LLM for
fine-grained training in MR environments. In addition, this shows
the proposed LEGO-MRTA dataset contains distinct characteristics
that have not been captured by the existing publicly available datasets.


FIGURE


Second, there exists a trade-off between overlap and informativeness
evaluation. For example, for metrics concerning overlap, Qwen
consistently achieves the highest scores; while the informativeness is inferior
to best models, i.e., ToolACC is 6.83% lower than that of XVERSE,
ThemeACC 10.73% lower than that of OpenChat3.5.


Third, the choice of backbone LLMs inherently impacts the performance
of both overlap and informativeness. We compute the standard
deviation for each metric over all models to gauge the variability in
performance scores: BLEU-4 (19.60), ROUGE1 (17.79), ROUGE-2 (16.02),
ROUGE-L (20.64), ToolACC (19.85), and ThemeACC (19.19).


Case Study


Figure *REF* shows an example to intuitively verify the
feasibility of generating conversation based on an instruction manual.
As highlighted in the trainer&apos;s utterance, &quot;Place the upper body on
the legs&quot;, this accurately conveys the instruction from the manual in
a human-like manner. The generated conversation is feasible at
instruction-following capability.


Figure *REF* illustrates an example demonstrating the
feasibility of constructing queries based on instructions from a
manual to accurately request positions in a multimodal context. We
transferred the generated position and highlighted a frame with
markers. We observed that the prediction was relatively accurate.
Additionally, another aspect we observe that needs improvement in the
future is the redundant output from the VLM.


FIGURE


Discussion of Broader Impact


The research presented in this paper offers a fully new environment to
advance how workers are trained and get help by using MR technologies.
By integrating AI assistants into MR environments, workers can tackle
complex tasks more effectively. This innovation not only enhances
worker productivity but also reduces training costs for companies,
as it eliminates the need for expert instructors to be physically
present for employee training sessions.


Conclusion


In this work, we introduce an autonomous workflow to develop smarter
multimodal fine-grained training assistants in MR environment.
Specifically, we have designed a cerebral language agent that
integrates LLM with memory, planning, and interaction with MR tools,
along with a vision-language agent. This seamless integration enables
agents to make decisions based on past experiences, thereby addressing
the challenge of tailoring assistant services through grounded
interactions. We have designed a vision-language agent to better
understand users&apos; situated multimodal contexts. Notably, we have
created a dataset for fine-grained training in MR. We have compared
the performance of openresource LLMs before and after fine-tuning
using this dataset. We aim to facilitate the development of smarter
assistants for seamless user interaction, fostering research in AI and HCI communities.